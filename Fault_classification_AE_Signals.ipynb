{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import mlflow.pytorch as mlp\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data from pickle file containing the AE signals\n",
    "original_ae = pd.read_pickle(\"./Pickle_files/ae_all_signals_july.pkl\")\n",
    "\n",
    "#remove columns where enough segment data is not present\n",
    "mask = (original_ae['AE Signal RMS Abrichten Sp. 51'].str.len() > 500) & (original_ae['AE Signal RMS Abrichten Sp. 51'].str.len() < 750)\n",
    "original_ae = original_ae.loc[mask]\n",
    "\n",
    "original_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the diameter is stored as array of string, needs to be converted to array of floats\n",
    "original_ae['Durchmesser Min. F端bo 1'] = original_ae['Durchmesser Min. F端bo 1'].str[0].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column in dataframe stating whether a part is scrap or non-scrap\n",
    "def label_race (row):\n",
    "    if ((row['Durchmesser Min. F端bo 1'] >= 0.0008) | (row['Durchmesser Min. F端bo 1'] <= -0.0008)):\n",
    "          return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "original_ae['is_scrap'] = original_ae.apply (lambda row: label_race(row), axis=1)\n",
    "original_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert array of strings of AE signals to array of floats\n",
    "original_ae[\"AE Signal RMS Abrichten Sp. 51\"] = original_ae[\"AE Signal RMS Abrichten Sp. 51\"].apply(lambda x : np.array([float(i) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample rows to get equal scrap and non-scrap samples \n",
    "scrap_rows = original_ae.loc[original_ae['is_scrap'] == 1.0]\n",
    "good_rows = original_ae.loc[original_ae['is_scrap'] == 0.0]   \n",
    "\n",
    "sample_good_rows = good_rows.sample(frac=0.4, replace=True, random_state=1)\n",
    "ae_aligned = pd.concat([scrap_rows, sample_good_rows])\n",
    "ae_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing the AE signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tSignal):\n",
    "    # copy the data if needed, omit and rename function argument if desired\n",
    "    signal = np.copy(tSignal) # signal is in range [a;b]\n",
    "    signal -= np.min(signal) # signal is in range to [0;b-a]\n",
    "    signal /= np.max(signal) # signal is normalized to [0;1]\n",
    "    signal -= 0.5 # signal is in range [-0.5;0.5]\n",
    "    signal *=2 # signal is in range [-1;1]\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "scrap_labels = []\n",
    "arr = []\n",
    "\n",
    "for ae_signal, scrap_label in zip(ae_aligned[\"AE Signal RMS Abrichten Sp. 51\"], ae_aligned.is_scrap): \n",
    "    if len(ae_signal) < 625:\n",
    "        counter = counter +1\n",
    "        continue \n",
    "    else:\n",
    "        #arr.append(normalize(ae_signal[0:625]))\n",
    "        arr.append(ae_signal[0:625])\n",
    "        scrap_labels.append(scrap_label)\n",
    "            \n",
    "arr2 = np.array(arr, dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr2.shape, len(scrap_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 channels for input of CNN model \n",
    "def mean(data,no_elements):\n",
    "    X=np.zeros((data.shape[0],data.shape[1]))\n",
    "    for i in range(data.shape[1]-no_elements+1):\n",
    "        X[:,i]=np.mean(data[:,i:i+no_elements],axis=1)\n",
    "    return X.astype(np.float16)\n",
    "def median(data,no_elements):\n",
    "    X=np.zeros((data.shape[0],data.shape[1]))\n",
    "    for i in range(data.shape[1]-no_elements+1):\n",
    "        X[:,i]=np.median(data[:,i:i+no_elements],axis=1)\n",
    "    return X.astype(np.float16)\n",
    "def sig_image(data,size1, size2):\n",
    "    X=np.zeros((data.shape[0],size1, size2))\n",
    "    for i in range(data.shape[0]):\n",
    "        X[i]=(data[i,:].reshape(size1, size2))\n",
    "    return X.astype(np.float16)\n",
    "\n",
    "\n",
    "channel_mean=(mean(arr2,10)).astype(np.float16)\n",
    "x_m=sig_image(channel_mean,25,25)\n",
    "channel_median=(median(arr2,10)).astype(np.float16)\n",
    "x_md=sig_image(arr2,25,25)\n",
    "x_n = sig_image(arr2,25,25)\n",
    "\n",
    "\n",
    "X=np.stack((x_n,x_m,x_md),axis=1).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scrap_labels = np.array(scrap_labels)\n",
    "trainx, testx, trainlabel, testlabel = train_test_split(X, scrap_labels, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_train, sig_test = trainx,testx\n",
    "lab_train, lab_test = trainlabel,testlabel\n",
    "\n",
    "sig_train = torch.from_numpy(sig_train)\n",
    "sig_test = torch.from_numpy(sig_test)\n",
    "lab_train= torch.from_numpy(lab_train)\n",
    "lab_test = torch.from_numpy(lab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "from torchsampler import ImbalancedDatasetSampler \n",
    "\n",
    "batch_size = 10 \n",
    "train_tensor = data_utils.TensorDataset(sig_train, lab_train) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, sampler = ImbalancedDatasetSampler(train_tensor, trainlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "test_tensor = data_utils.TensorDataset(sig_test, lab_test) \n",
    "test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sig_train.size(), sig_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise the signals if not done before\n",
    "class Z_Normalisation(nn.Module):\n",
    "    def __call__(self, tensor):\n",
    "        #val = (tensor - tensor[0].mean()/ tensor[0].std()) #std normalisation -- option1\n",
    "        return (tensor-torch.min(tensor))/(torch.max(tensor)-torch.min(tensor)) #min-max normalization -- option2\n",
    "        #return val\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.sequential_stack = nn.Sequential(\n",
    "            #Z_Normalisation(),\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(kernel_size=4),\n",
    "        )\n",
    "\n",
    "        self.fully_connected_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=256, out_features=256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=256, out_features=64, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=64, out_features=2, bias=True),\n",
    "            #nn.LogSoftmax()\n",
    "        )\n",
    "        self.gradients = None\n",
    "        #self.classifier = nn.Linear(2 + 7) # 2 out-features and 7 hand made features\n",
    "        \n",
    "        \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "        # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        return self.sequential_stack(x)\n",
    "\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.sequential_stack(x)\n",
    "        if y.requires_grad:\n",
    "            h = y.register_hook(self.activations_hook)\n",
    "\n",
    "        logits=self.fully_connected_stack(y)       \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted loss function when data is not sampled\n",
    "#weights = torch.tensor([1.9856, 98.0144], dtype=torch.double)\n",
    "\n",
    "# cross-entropy loss & Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500 #set epochs\n",
    "\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "with mlflow.start_run() as run:\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (signals, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # Run the forward pass\n",
    "            outputs = cnn(signals.double())\n",
    "            loss = criterion(outputs, labels.long())\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            # Backprop and perform Adam optimisation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Track the accuracy\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels.long()).sum().item()\n",
    "            acc_list.append(correct / total)\n",
    "\n",
    "            if (epoch+1) % 5 == 0 or epoch==0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Train Accuracy: {:.2f}%'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                              (correct / total) * 100))\n",
    "                \n",
    "            mlph.log(\"loss\", loss, on_epoch=True)\n",
    "            mlp.log(\"train_acc\", (correct / total), on_epoch=True)\n",
    "            mlp.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get average accuracy\n",
    "avg_acc = np.mean(acc_list)\n",
    "avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the test set to check the model accuracy\n",
    "total_step = len(test_loader)\n",
    "print(total_step)\n",
    "loss_list_test = []\n",
    "acc_list_test = []\n",
    "with torch.no_grad():\n",
    "    for i, (signals, labels) in enumerate(test_loader):\n",
    "        # Run the forward pass\n",
    "        signals=signals\n",
    "        labels=labels\n",
    "        outputs = cnn(signals.double())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss_list_test.append(loss.item())\n",
    "        if epoch%10 ==0:\n",
    "            print(loss)\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels.long()).sum().item()\n",
    "        acc_list_test.append(correct / total)\n",
    "        if (epoch) % 1 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix of binary classification (CNN Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\"Scrap\", \"Non-Scrap\"}\n",
    "\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "        output = cnn(inputs.double())\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # Save Truth\n",
    "\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "#df_cm = pd.DataFrame(cf_matrix , index = [i for i in classes],\n",
    "#                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(cf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcualte F1-score\n",
    "f1_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction of AE signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thresholding Algorithm Source: \n",
    "#https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data/\n",
    "\n",
    "def thresholding_algo(y, lag = 2, threshold = 3, influence = 0.0):\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    \n",
    "    for i in range(lag, len(y)):\n",
    "        if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter [i-1]:\n",
    "            if y[i] > avgFilter[i-1]+0.3: \n",
    "                signals[i] = 1 #peak\n",
    "            filteredY[i] = influence * y[i] + (1 - influence) * filteredY[i-1]\n",
    "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "        else:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return np.asarray(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_index = []\n",
    "def calculate_features(signal, idx):\n",
    "\n",
    "    result = thresholding_algo(signal) \n",
    "    \n",
    "    # Get start, stop index pairs for islands/seq. of 1s\n",
    "    idx_pairs = np.where(np.diff(np.hstack(([False],result==1,[False]))))[0].reshape(-1,2)\n",
    "    \n",
    "    if len(idx_pairs) == 0:\n",
    "        weird_index.append(idx)\n",
    "        return [None]\n",
    "    \n",
    "    # Get the island lengths, whose argmax would give us the ID of longest island.\n",
    "    # Start index of that island would be the desired output\n",
    "    start_longest_seq = idx_pairs[np.diff(idx_pairs,axis=1).argmax(),0]\n",
    "    \n",
    "    #index of the largest value\n",
    "    highest_index = np.argmax(signal)\n",
    "    \n",
    "    #calculate angle of slope\n",
    "    dx = (highest_index/len(signal) - start_longest_seq/len(signal))\n",
    "\n",
    "    # Difference in y coordinates\n",
    "    dy = 1 - signal[start_longest_seq]/signal[highest_index]\n",
    "\n",
    "    # Angle between p1 and p2 in radians\n",
    "    theta = math.atan2(dy, dx)\n",
    "    \n",
    "    #highest value\n",
    "    max_value = signal[highest_index]\n",
    "\n",
    "    #area under the curve\n",
    "    area_under_curve = np.trapz(signal[signal < max_value])\n",
    "    \n",
    "    #last contact point\n",
    "    occurences = np.where(result == 1)\n",
    "    last_contact = occurences[0][-1]\n",
    "    \n",
    "    max_slope = max([x - z for x, z in zip(signal[:-1], signal[1:])])\n",
    "    #print(signal[start_longest_seq], theta, max_value, area_under_curve, signal[last_contact], max_slope)\n",
    "    return [signal[start_longest_seq], theta, max_value, area_under_curve, signal[last_contact], max_slope] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def feature_extract(data, size):\n",
    "    features=np.zeros((data.shape[0], size))\n",
    "    for i in range(data.shape[0]):\n",
    "        features[i] = calculate_features(data[i], i)\n",
    "    return features.astype(np.float16)\n",
    "\n",
    "features_arr = feature_extract(data = arr2, size = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for State-of-the-art algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1200)\n",
    "features = pd.DataFrame(list(map(np.ravel, features_arr)))\n",
    "features.columns = ['first contact point', 'Theta', 'max value', 'area under curve', 'last contact point', 'slope']\n",
    "features['is_scrap'] = ae_aligned[\"is_scrap\"].to_list()\n",
    "#features['Part_ID'] = ae_aligned.index.to_list()\n",
    "X = features.dropna()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import for State-of-the-art algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X.loc[:, X.columns != 'is_scrap'])\n",
    "X_norm = scaler.transform(X.loc[:, X.columns != 'is_scrap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Models from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_values = []\n",
    "F1_score_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier(n_estimators=10,random_state=10)\n",
    "score = cross_val_score(RF, X_norm, X['is_scrap'], cv=10) #, scoring = 'f1' -- use for generating F1 score\n",
    "mean_accuracy = np.mean(np.array(score))\n",
    "print('Fold-wise accuracies: ', score)\n",
    "print('Mean accuracy: ', mean_accuracy)\n",
    "accuracy_values.append(mean_accuracy)\n",
    "#F1_score_values.append(mean_accuracy)# when scoring = 'f1' is activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=LogisticRegression(random_state=6, solver='lbfgs',multi_class='multinomial')\n",
    "score = cross_val_score(LR, X_norm, X['is_scrap'], cv=10) #, scoring = 'f1'\n",
    "mean_accuracy = np.mean(np.array(score))\n",
    "print('Fold-wise accuracies: ', score)\n",
    "print('Mean accuracy: ', mean_accuracy)\n",
    "accuracy_values.append(mean_accuracy)\n",
    "#F1_score_values.append(mean_accuracy)# when scoring = 'f1' is activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=SVC(random_state=100, tol=1e-1) \n",
    "score = cross_val_score(svc, X_norm, X['is_scrap'], cv=5 )#, scoring = 'f1'\n",
    "mean_accuracy = np.mean(np.array(score))\n",
    "print('Fold-wise accuracies: ', score)\n",
    "print('Mean accuracy: ', mean_accuracy)\n",
    "accuracy_values.append(mean_accuracy)\n",
    "#F1_score_values.append(mean_accuracy)# when scoring = 'f1' is activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=LogisticRegression(random_state=6, solver='lbfgs',multi_class='multinomial')\n",
    "score = cross_val_score(LR, X_norm, X['is_scrap'], cv=10) #, scoring = 'f1'\n",
    "mean_accuracy = np.mean(np.array(score))\n",
    "print('Fold-wise accuracies: ', score)\n",
    "print('Mean accuracy: ', mean_accuracy)\n",
    "accuracy_values.append(mean_accuracy)\n",
    "#F1_score_values.append(mean_accuracy)# when scoring = 'f1' is activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=25)\n",
    "score = cross_val_score(knn, X_norm, X['is_scrap'], cv=10)#, scoring = 'f1'\n",
    "mean_accuracy = np.mean(np.array(score))\n",
    "print('Fold-wise accuracies: ', score)\n",
    "print('Mean accuracy: ', mean_accuracy)\n",
    "accuracy_values.append(mean_accuracy)\n",
    "#F1_score_values.append(mean_accuracy)# when scoring = 'f1' is activated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracy and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "#plt.figure(figsize = (15,7))\n",
    "width = 0.35\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "algos = ['Random Forest Classifier', 'SVM', 'Logistic Regression', 'KNN Classifier', 'Multilayer Perceptron']\n",
    "\n",
    "ax.set_ylabel('Mean Performance (%)')\n",
    "ax.set_title('Performance of Classification Algorithms')\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "plt.xticks(rotation=90)\n",
    "acc = ax.bar(algos,accuracy_values, width, color='b')\n",
    "f1 = ax.bar(algos,F1_score_values, width, color='g')\n",
    "\n",
    "ax.legend( (acc, f1), ('accuracy', 'F1 score') )\n",
    "\n",
    "'''for i, v in enumerate(accuracy_values):\n",
    "    ax.text(i-.15, \n",
    "              v/accuracy_values[i]+82, \n",
    "              accuracy_values[i], \n",
    "              fontsize=5, \n",
    "              color='black')'''\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-POC]",
   "language": "python",
   "name": "conda-env-.conda-POC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
